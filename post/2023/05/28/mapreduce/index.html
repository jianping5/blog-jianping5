<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>MapReduce | Jianping5</title>
    <meta name="generator" content="VuePress 1.9.9">
    <link rel="icon" href="favicon.jpg">
    <meta name="description" content="
Introduction

The major contributions of this work are a simple and powerful interface that enables automatic parallelization and distribution of large-scale computations, combined with an implem ...">
    <meta name="theme-color" content="#3eaf7c">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    
    <link rel="preload" href="/assets/css/0.styles.1d43d066.css" as="style"><link rel="preload" href="/assets/js/app.b266aa11.js" as="script"><link rel="preload" href="/assets/js/7.095168a6.js" as="script"><link rel="preload" href="/assets/js/3.5695e672.js" as="script"><link rel="preload" href="/assets/js/11.b408beb5.js" as="script"><link rel="prefetch" href="/assets/js/10.2df2f488.js"><link rel="prefetch" href="/assets/js/12.6cdaaa90.js"><link rel="prefetch" href="/assets/js/13.196d23b8.js"><link rel="prefetch" href="/assets/js/14.65c5c5f0.js"><link rel="prefetch" href="/assets/js/15.609769d2.js"><link rel="prefetch" href="/assets/js/16.6855a45b.js"><link rel="prefetch" href="/assets/js/17.2afda71f.js"><link rel="prefetch" href="/assets/js/18.66524d53.js"><link rel="prefetch" href="/assets/js/19.a665d640.js"><link rel="prefetch" href="/assets/js/20.afc77843.js"><link rel="prefetch" href="/assets/js/21.cd24cdbb.js"><link rel="prefetch" href="/assets/js/4.111f6883.js"><link rel="prefetch" href="/assets/js/5.6b1dfe39.js"><link rel="prefetch" href="/assets/js/6.fcbd1a3f.js"><link rel="prefetch" href="/assets/js/8.b2a0309d.js"><link rel="prefetch" href="/assets/js/9.d6c89be3.js"><link rel="prefetch" href="/assets/js/vuejs-paginate.0df4708d.js">
    <link rel="stylesheet" href="/assets/css/0.styles.1d43d066.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div id="vuepress-theme-blog__global-layout"><section id="header-wrapper"><header id="header"><div class="header-wrapper"><div class="title"><a href="/" class="nav-link home-link">Jianping5 </a></div> <div class="header-right-wrap"><ul class="nav"><li class="nav-item"><a href="/post/" class="nav-link router-link-active">Blog</a></li><li class="nav-item"><a href="/about/" class="nav-link">About</a></li><li class="nav-item"><a href="https://github.com/jianping5" target="_blank" rel="noopener noreferrer" class="nav-link external">Github</a></li></ul> <div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <!----></div></div></header></section> <div id="mobile-header"><div class="mobile-header-bar"><div class="mobile-header-title"><a href="/" class="nav-link mobile-home-link">Jianping5 </a> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"><line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line></svg></div> <div class="mobile-menu-wrapper"><hr class="menu-divider"> <ul class="mobile-nav"><li class="mobile-nav-item"><a href="/post/" class="nav-link router-link-active">Blog</a></li><li class="mobile-nav-item"><a href="/about/" class="nav-link">About</a></li><li class="mobile-nav-item"><a href="https://github.com/jianping5" target="_blank" rel="noopener noreferrer" class="nav-link external">Github</a></li> <li class="mobile-nav-item"><!----></li></ul></div></div></div> <div class="content-wrapper"><div id="vuepress-theme-blog__post-layout"><article itemscope="itemscope" itemtype="https://schema.org/BlogPosting" class="vuepress-blog-theme-content"><header><h1 itemprop="name headline" class="post-title">
        MapReduce
      </h1> <div class="post-meta"><div itemprop="publisher author" itemtype="http://schema.org/Person" itemscope="itemscope" class="post-meta-author"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-navigation"><polygon points="3 11 22 2 13 21 11 13 3 11"></polygon></svg> <span itemprop="name">jianping5</span> <span itemprop="address">   in ShangHai</span></div> <div class="post-meta-date"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-clock"><circle cx="12" cy="12" r="10"></circle><polyline points="12 6 12 12 16 14"></polyline></svg> <time pubdate itemprop="datePublished" datetime="2023-05-28T00:00:00.000Z">
      Sun May 28 2023
    </time></div> <!----></div></header> <div itemprop="articleBody" class="content__default"><h2 id="abstract"><a href="#abstract" class="header-anchor">#</a> Abstract</h2> <h2 id="introduction"><a href="#introduction" class="header-anchor">#</a> Introduction</h2> <p>The major contributions of this work are a simple and powerful interface that enables automatic parallelization and distribution of large-scale computations, combined with an implementation of this interface that achieves high performance on large clusters of commodity PCs.</p> <h2 id="programming-model"><a href="#programming-model" class="header-anchor">#</a> Programming Model</h2> <p><img src="/assets/img/1.dc0581d0.png" alt="1.png"></p> <h2 id="implementation"><a href="#implementation" class="header-anchor">#</a> Implementation</h2> <h3 id="execution-overview"><a href="#execution-overview" class="header-anchor">#</a> Execution Overview</h3> <p>Figure 1 shows the overall flow of a MapReduce operation in our implementation. When the user program</p> <p>calls the MapReduce function, the following sequence of actions occurs (the numbered labels in Figure 1 correspond to the numbers in the list below):</p> <ol><li>The MapReduce library in the user program first splits the input files into M pieces of typically 16 megabytes to 64 megabytes (MB) per piece (controllable by the user via an optional parameter). It then starts up many copies of the program on a cluster of machines.</li> <li>One of the copies of the program is special – the master. The rest are workers that are assigned work by the master. There are M map tasks and R reduce tasks to assign. The master picks idle workers and assigns each one a map task or a reduce task.</li> <li>A worker who is assigned a map task reads the contents of the corresponding input split. It parses key/value pairs out of the input data and passes each pair to the user-defined Map function. The intermediate key/value pairs produced by the Map function are buffered in memory.</li> <li>Periodically, the buffered pairs are written to local disk, partitioned into R regions by the partitioning function. The locations of these buffered pairs on the local disk are passed back to the master, who is responsible for forwarding these locations to the reduce workers.</li> <li>When a reduce worker is notified by the master about these locations, it uses remote procedure calls to read the buffered data from the local disks of the map workers. When a reduce worker has read all intermediate data, it sorts it by the intermediate keys so that all occurrences of the same key are grouped together. The sorting is needed because typically many different keys map to the same reduce task. If the amount of intermediate data is too large to fit in memory, an external sort is used.</li> <li>The reduce worker iterates over the sorted intermediate data and for each unique intermediate key encountered, it passes the key and the corresponding set of intermediate values to the user’s Reduce function. The output of the Reduce function is appended to a final output file for this reduce partition.</li> <li>When all map tasks and reduce tasks have been completed, the master wakes up the user program. At this point, the MapReduce call in the user program returns back to the user code.</li></ol> <h3 id="master-data-structures"><a href="#master-data-structures" class="header-anchor">#</a> Master Data Structures</h3> <p>For each map，task and reduce task, it stores the state (idle, in-progress, or completed), and the identity of the worker machine (for non-idle tasks).</p> <h3 id="fault-tolerance"><a href="#fault-tolerance" class="header-anchor">#</a> Fault Tolerance</h3> <p><strong>Worker Failure</strong></p> <p>The master pings every worker periodically.</p> <p>Who needs to be reset to idle and becomes eligible for rescheduling?</p> <ul><li>Any map tasks completed (because their output is stored on the local disk(s) of the failed machine and is therefore inaccessible)</li> <li>Any map task or reduce task in progress</li></ul> <blockquote><p>Completed reduce tasks do not need to be re-executed since their output is stored in a global file system.</p></blockquote> <p><strong>Master Failure</strong></p> <p>It is easy to make the master write periodic checkpoints of the master data structures described above.</p> <p><strong>Semantics in the Presence of Failures</strong></p> <p>When the user-supplied map and reduce operators are <strong>deterministic</strong> functions of their input values, our distributed implementation produces the same output as would have been produced by a non-faulting sequential execution of the entire program.</p> <p>We rely on <strong>atomic commits</strong> of map and reduce task outputs to achieve this property.</p> <h3 id="locality"><a href="#locality" class="header-anchor">#</a> Locality</h3> <p>Network bandwidth is a relatively scarce resource in our computing environment.</p> <p>We conserve network bandwidth by taking advantage of the fact that the input data (managed by GFS [8]) is stored on the local disks of the machines that make up our cluster.</p> <h3 id="task-granularity"><a href="#task-granularity" class="header-anchor">#</a> Task Granularity</h3> <p>We subdivide the map phase into M pieces and the reduce phase into R pieces, as described above.</p> <h3 id="backup-tasks"><a href="#backup-tasks" class="header-anchor">#</a> Backup Tasks</h3> <h2 id="refinements"><a href="#refinements" class="header-anchor">#</a> Refinements</h2> <h3 id="partitioning-function"><a href="#partitioning-function" class="header-anchor">#</a> Partitioning Function</h3> <p>The users of MapReduce specify the number of reduce tasks/output files that they desire (R).</p> <h3 id="ordering-guarantees"><a href="#ordering-guarantees" class="header-anchor">#</a> Ordering Guarantees</h3> <p>This ordering guarantee makes it easy to generate a sorted output file per partition,</p> <h3 id="combiner-function"><a href="#combiner-function" class="header-anchor">#</a> Combiner Function</h3> <p>We allow the user to specify an optional Combiner function that does partial merging of this data before it is sent over the network.</p> <p>The Combiner function is executed on each machine that performs a map task.</p> <p>What is the difference between a reduce function and a combiner function？</p> <p>The only difference between a reduce function and a combiner function is how the MapReduce library handles the output of the function.</p> <ul><li>The output of a reduce function is written to the final output file.</li> <li>The output of a combiner function is written to an intermediate file that will be sent to a reduce task.</li></ul> <p>Partial combining significantly speeds up certain classes of MapReduce operations.</p> <h3 id="input-and-output-types"><a href="#input-and-output-types" class="header-anchor">#</a> Input and Output Types</h3> <p>The MapReduce library provides support for reading input data in several different formats.</p> <h3 id="side-effects"><a href="#side-effects" class="header-anchor">#</a> Side-effects</h3> <p>In some cases, users of MapReduce have found it convenient to produce auxiliary files as additional outputs from their map and/or reduce operators.</p> <p>We do not provide support for atomic two-phase commits of multiple output files produced by a single task.</p> <h3 id="skipping-bad-records"><a href="#skipping-bad-records" class="header-anchor">#</a> Skipping Bad Records</h3> <p>We provide an optional mode of execution where the MapReduce library detects which records cause deterministic crashes and skips these records in order to make forward progress.</p> <h3 id="local-execution"><a href="#local-execution" class="header-anchor">#</a> Local Execution</h3> <p>To help facilitate debugging, profiling, and small-scale testing, we have developed an alternative implementation of the MapReduce library that sequentially executes all of the work for a MapReduce operation on the local machine.</p> <h3 id="status-information"><a href="#status-information" class="header-anchor">#</a> Status Information</h3> <p>The master runs an internal HTTP server and exports a set of status pages for human consumption.</p> <h3 id="counters"><a href="#counters" class="header-anchor">#</a> Counters</h3> <p>The MapReduce library provides a <strong>counter facility</strong> to count occurrences of various events.</p> <p>The counter values from individual worker machines are periodically propagated to the master (piggybacked on the ping response). The master aggregates the counter values from successful map and reduce tasks and returns them to the user code when the MapReduce operation is completed.</p> <p>Users have found the counter facility useful for sanity checking the behavior of MapReduce operations.</p> <h2 id="performance"><a href="#performance" class="header-anchor">#</a> Performance</h2> <p>In this section we measure the performance of MapReduce on two computations running on a large cluster of machines. One computation searches through approximately one terabyte of data looking for a particular pattern. The other computation sorts approximately one terabyte of data.</p> <h2 id="experience"><a href="#experience" class="header-anchor">#</a> Experience</h2> <h3 id="large-scale-indexing"><a href="#large-scale-indexing" class="header-anchor">#</a> Large-Scale Indexing</h3> <p>One of our most significant uses of MapReduce to date has been a complete rewrite of the production indexing system that produces the data structures used for the Google web search service.</p> <h2 id="related-work"><a href="#related-work" class="header-anchor">#</a> Related Work</h2> <blockquote><p>This chapter introduces the related work and makes a comparison between this work and other related work.</p></blockquote> <p>Many systems have provided restricted programming models and used the restrictions to parallelize the computation automatically.</p> <p>More significantly, we provide a fault-tolerant implementation that scales to thousands of processors.</p> <p>In contrast, most of the parallel processing systems have only been implemented on smaller scales and leave the details of handling machine failures to the programmer.</p> <h2 id="conclusions"><a href="#conclusions" class="header-anchor">#</a> Conclusions</h2> <p>The MapReduce programming model has been successfully used at Google for many different purposes.</p> <ul><li>First, the model is easy to use, even for programmers without experience with parallel and distributed systems, since it hides the details of parallelization, fault-tolerance, locality optimization, and load balancing.</li> <li>Second, a large variety of problems are easily expressible as MapReduce computations.</li> <li>Third, we have developed an implementation of MapReduce that scales to large clusters of machines comprising thousands of machines.</li></ul> <p>We have learned several things from this work.</p> <ul><li>First, restricting the programming model makes it easy to parallelize and distribute computations and to make such computations fault-tolerant.</li> <li>Second, network bandwidth is a scarce resource.</li></ul> <blockquote><p>A number of optimizations in our system are therefore targeted at reducing the amount of data sent across the network: the locality optimization allows us to read data from local disks, and writing a single copy of the intermediate data to local disk saves network bandwidth.</p></blockquote> <ul><li>Third, redundant execution can be used to reduce the impact of slow machines, and to handle machine failures and data loss.</li></ul> <h2 id="acknowledgements"><a href="#acknowledgements" class="header-anchor">#</a> Acknowledgements</h2> <h2 id="references"><a href="#references" class="header-anchor">#</a> References</h2> <h2 id="references-2"><a href="#references-2" class="header-anchor">#</a> References'</h2> <ol><li>Video: https://www.youtube.com/watch?v=WtZ7pcRSkOA&amp;feature=youtu.be</li> <li>Lecture Notes: http://nil.csail.mit.edu/6.824/2022/notes/l01.txt</li> <li>Paper: http://nil.csail.mit.edu/6.824/2022/papers/mapreduce.pdf</li></ol> <h2 id="q-a"><a href="#q-a" class="header-anchor">#</a> Q &amp; A</h2> <p>e.g. Word count</p> <ol><li>Input is already split into M files</li> <li>MR calls Map() for each input file, produces sets of k2, v2 &quot;intermediate&quot; data</li></ol> <blockquote><p>Each Map() call is a &quot;task&quot;</p></blockquote> <ol><li>When Maps are done, MR gathers all intermediate v2's for a given k2, and passes each key + values to a Reduce call</li> <li>Final output is sets of &lt;k2,v3&gt; pairs from Reduce()s</li></ol> <p><strong>MapReduce hides many details</strong></p> <ol><li>Sending app code to servers</li> <li>Tracking which tasks have finished</li> <li>&quot;shuffling&quot; intermediate data from Maps to Reduces</li> <li>Balancing load over servers</li> <li>Recovering from failures</li></ol> <p><strong>However, MapReduce limits what apps can do</strong></p> <ol><li>No interaction or state (other than via intermediate output).</li> <li>No iteration</li> <li>No real-time or streaming processing.</li></ol> <p><strong>Input and output are stored on the GFS cluster file system</strong></p> <ol><li>MR needs huge parallel input and output throughput.</li> <li>GFS splits files over many servers, in 64 MB chunks</li> <li>Maps read in parallel</li> <li>Reduces write in parallel</li> <li>GFS also replicates each file on 2 or 3 servers</li> <li>GFS is a big win for MapReduce</li></ol> <p><strong>Some details (paper's Figure 1)</strong></p> <p>one coordinator, that hands out tasks to workers and remembers progress.</p> <ol><li>coordinator gives Map tasks to workers until all Maps complete     Maps write output (intermediate data) to local disk     Maps split output, by hash, into one file per Reduce task</li> <li>After all Maps have finished, coordinator hands out Reduce tasks     each Reduce fetches its intermediate output from (all) Map workers     each Reduce task writes a separate output file on GFS</li></ol> <p><strong>What will likely limit the performance?</strong></p> <p>In 2004 authors were limited by network capacity. What does MR send over the network?</p> <ol><li>Maps read input from GFS.</li> <li>Reduces read Map intermediate output.</li> <li>Often as large as input, e.g. for sorting.</li> <li>Reduces write output files to GFS.</li> <li>[diagram: servers, tree of network switches]</li> <li>In MR's all-to-all shuffle, half of traffic goes through root switch. Paper's root switch: 100 to 200 gigabits/second, total 1800 machines, so 55 megabits/second/machine. 55 is small:  much less than disk or RAM speed.  Today: networks are much faster</li></ol> <p><strong>How does MR minimize network use?</strong></p> <ul><li><p>Coordinator tries to run each Map task on GFS server that stores its input.</p></li> <li><p>All computers run both GFS and MR workers.</p></li> <li><p>So input is read from local disk (via GFS), not over network.</p></li></ul> <p>Intermediate data goes over network just once.</p> <p>Map worker writes to local disk.</p> <p>Reduce workers read from Map worker disks over the network.</p> <p>Storing it in GFS would require at least two trips over the network.</p> <p>Intermediate data partitioned into files holding many keys.</p> <p>R is much smaller than the number of keys.</p> <p>Big network transfers are more efficient.</p> <p><strong>How does MR get good load balance?</strong></p> <p>Wasteful and slow if N-1 servers have to wait for 1 slow server to finish.</p> <p>But some tasks likely take longer than others.</p> <p>Solution: many more tasks than workers.    Coordinator hands out new tasks to workers who finish previous tasks.    So no task is so big it dominates completion time (hopefully).    So faster servers do more tasks than slower ones, finish abt the same time.</p> <p><strong>What about fault tolerance?</strong></p> <p>MR re-runs just the failed Map()s and Reduce()s.</p> <ul><li><p>Suppose MR runs a Map twice, one Reduce sees first run's output, another Reduce sees the second run's output?</p></li> <li><p>Correctness requires re-execution to yield exactly the same output.</p></li> <li><p>So Map and Reduce must be pure deterministic functions: they are only allowed to look at their      arguments/input. no state, no file I/O, no interaction, no external communication.</p></li> <li><p>What if you wanted to allow non-functional Map or Reduce?</p></li> <li><p>Worker failure would require whole job to be re-executed, or you'd need to roll back to some kind of global checkpoint.</p></li></ul> <p><strong>Details of worker crash recovery</strong></p> <p>a Map worker crashes:</p> <p>coordinator notices worker no longer responds to pings</p> <p>coordinator knows which Map tasks ran on that worker</p> <p>those tasks' intermediate output is now lost, must be re-created</p> <p>coordinator tells other workers to run those tasks</p> <p>can omit re-running if all Reduces have fetched the intermediate data</p> <p>a Reduce worker crashes:</p> <p>finished tasks are OK -- stored in GFS, with replicas.</p> <p>coordinator re-starts worker's unfinished tasks on other workers.</p> <p>Other failures/problems</p> <ul><li>What if the coordinator gives two workers the same Map() task?
perhaps the coordinator incorrectly thinks one worker died.
it will tell Reduce workers about only one of them.</li> <li>What if the coordinator gives two workers the same Reduce() task?
they will both try to write the same output file on GFS!
atomic GFS rename prevents mixing; one complete file will be visible.</li> <li>What if a single worker is very slow -- a &quot;straggler&quot;?
perhaps due to flakey hardware.
coordinator starts a second copy of last few tasks.</li> <li>What if a worker computes incorrect output, due to broken h/w or s/w?
too bad! MR assumes &quot;fail-stop&quot; CPUs and software.</li> <li>What if the coordinator crashes?</li></ul> <p>Current status?
Hugely influential (Hadoop, Spark, &amp;c).
Probably no longer in use at Google.
Replaced by Flume / FlumeJava (see paper by Chambers et al).
GFS replaced by Colossus (no good description), and BigTable.</p></div> <footer><!----> <hr> <!----></footer></article> <div class="sticker vuepress-toc"><div class="vuepress-toc-item vuepress-toc-h2 active"><a href="#abstract" title="Abstract">Abstract</a></div><div class="vuepress-toc-item vuepress-toc-h2"><a href="#introduction" title="Introduction">Introduction</a></div><div class="vuepress-toc-item vuepress-toc-h2"><a href="#programming-model" title="Programming Model">Programming Model</a></div><div class="vuepress-toc-item vuepress-toc-h2"><a href="#implementation" title="Implementation">Implementation</a></div><div class="vuepress-toc-item vuepress-toc-h3"><a href="#execution-overview" title="Execution Overview">Execution Overview</a></div><div class="vuepress-toc-item vuepress-toc-h3"><a href="#master-data-structures" title="Master Data Structures">Master Data Structures</a></div><div class="vuepress-toc-item vuepress-toc-h3"><a href="#fault-tolerance" title="Fault Tolerance">Fault Tolerance</a></div><div class="vuepress-toc-item vuepress-toc-h3"><a href="#locality" title="Locality">Locality</a></div><div class="vuepress-toc-item vuepress-toc-h3"><a href="#task-granularity" title="Task Granularity">Task Granularity</a></div><div class="vuepress-toc-item vuepress-toc-h3"><a href="#backup-tasks" title="Backup Tasks">Backup Tasks</a></div><div class="vuepress-toc-item vuepress-toc-h2"><a href="#refinements" title="Refinements">Refinements</a></div><div class="vuepress-toc-item vuepress-toc-h3"><a href="#partitioning-function" title="Partitioning Function">Partitioning Function</a></div><div class="vuepress-toc-item vuepress-toc-h3"><a href="#ordering-guarantees" title="Ordering Guarantees">Ordering Guarantees</a></div><div class="vuepress-toc-item vuepress-toc-h3"><a href="#combiner-function" title="Combiner Function">Combiner Function</a></div><div class="vuepress-toc-item vuepress-toc-h3"><a href="#input-and-output-types" title="Input and Output Types">Input and Output Types</a></div><div class="vuepress-toc-item vuepress-toc-h3"><a href="#side-effects" title="Side-effects">Side-effects</a></div><div class="vuepress-toc-item vuepress-toc-h3"><a href="#skipping-bad-records" title="Skipping Bad Records">Skipping Bad Records</a></div><div class="vuepress-toc-item vuepress-toc-h3"><a href="#local-execution" title="Local Execution">Local Execution</a></div><div class="vuepress-toc-item vuepress-toc-h3"><a href="#status-information" title="Status Information">Status Information</a></div><div class="vuepress-toc-item vuepress-toc-h3"><a href="#counters" title="Counters">Counters</a></div><div class="vuepress-toc-item vuepress-toc-h2"><a href="#performance" title="Performance">Performance</a></div><div class="vuepress-toc-item vuepress-toc-h2"><a href="#experience" title="Experience">Experience</a></div><div class="vuepress-toc-item vuepress-toc-h3"><a href="#large-scale-indexing" title="Large-Scale Indexing">Large-Scale Indexing</a></div><div class="vuepress-toc-item vuepress-toc-h2"><a href="#related-work" title="Related Work">Related Work</a></div><div class="vuepress-toc-item vuepress-toc-h2"><a href="#conclusions" title="Conclusions">Conclusions</a></div><div class="vuepress-toc-item vuepress-toc-h2"><a href="#acknowledgements" title="Acknowledgements">Acknowledgements</a></div><div class="vuepress-toc-item vuepress-toc-h2"><a href="#references" title="References">References</a></div><div class="vuepress-toc-item vuepress-toc-h2"><a href="#references-2" title="References'">References'</a></div><div class="vuepress-toc-item vuepress-toc-h2"><a href="#q-a" title="Q &amp; A">Q &amp; A</a></div></div></div></div> <footer class="footer" data-v-3d9deeb8><div class="footer-left-wrap" data-v-3d9deeb8><ul class="contact" data-v-3d9deeb8><li class="contact-item" data-v-3d9deeb8><a href="https://github.com/jianping5" target="_blank" rel="noopener noreferrer" class="nav-link external" data-v-3d9deeb8><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-github" data-v-3d9deeb8><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22" data-v-3d9deeb8></path></svg>
          
        </a></li><li class="contact-item" data-v-3d9deeb8><a href="mailto:jianping756@gmail.com" class="nav-link external" data-v-3d9deeb8><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-mail" data-v-3d9deeb8><path d="M4 4h16c1.1 0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1 0-2-.9-2-2V6c0-1.1.9-2 2-2z" data-v-3d9deeb8></path><polyline points="22,6 12,13 2,6" data-v-3d9deeb8></polyline></svg>
          
        </a></li></ul></div> <div class="footer-right-wrap" data-v-3d9deeb8><ul class="copyright" data-v-3d9deeb8><li class="copyright-item" data-v-3d9deeb8>jianping5 © 2023</li></ul></div></footer></div><div class="global-ui"></div></div>
    <script src="/assets/js/app.b266aa11.js" defer></script><script src="/assets/js/7.095168a6.js" defer></script><script src="/assets/js/3.5695e672.js" defer></script><script src="/assets/js/11.b408beb5.js" defer></script>
  </body>
</html>
