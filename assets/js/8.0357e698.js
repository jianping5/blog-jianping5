(window.webpackJsonp=window.webpackJsonp||[]).push([[8],{296:function(e,t,a){e.exports=a.p+"assets/img/1.ed939f98.png"},297:function(e,t,a){e.exports=a.p+"assets/img/2.c88a8c1a.png"},298:function(e,t,a){e.exports=a.p+"assets/img/3.9ac6bb42.png"},331:function(e,t,a){"use strict";a.r(t);var s=a(5),n=Object(s.a)({},(function(){var e=this,t=e._self._c;return t("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[t("h2",{attrs:{id:"design-overview"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#design-overview"}},[e._v("#")]),e._v(" Design Overview")]),e._v(" "),t("h3",{attrs:{id:"architecture"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#architecture"}},[e._v("#")]),e._v(" Architecture")]),e._v(" "),t("p",[t("img",{attrs:{src:a(296),alt:"1.png"}})]),e._v(" "),t("p",[e._v("A GFS consists of")]),e._v(" "),t("ul",[t("li",[e._v("A single master")]),e._v(" "),t("li",[e._v("Multiple chunkservers")]),e._v(" "),t("li",[e._v("Multiple clients")])]),e._v(" "),t("p",[e._v("Files are divided into fixed-size chunks.")]),e._v(" "),t("p",[e._v("The master maintains all file system "),t("strong",[e._v("metadata")]),e._v(".")]),e._v(" "),t("p",[e._v("GFS client code linked into each application implements the file system API and communicates with the master and chunkservers to read or write data on behalf of the application.")]),e._v(" "),t("p",[e._v("Clients interact with the master for metadata operations, but all data-bearing communication goes directly to the chunkservers.")]),e._v(" "),t("p",[t("strong",[e._v("Clients do cache metadata")]),e._v(".")]),e._v(" "),t("h3",{attrs:{id:"single-master"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#single-master"}},[e._v("#")]),e._v(" Single Master")]),e._v(" "),t("p",[e._v("Having a single master vastly simplifies our design and enables the master to make sophisticated chunk placement and replication decisions using global knowledge.")]),e._v(" "),t("h3",{attrs:{id:"chunk-size"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#chunk-size"}},[e._v("#")]),e._v(" Chunk Size")]),e._v(" "),t("p",[e._v("Lazy space allocation avoids wasting space due to internal fragmentation, perhaps the greatest objection against such a large chunk size.")]),e._v(" "),t("p",[e._v("A large chunk size offers several important advantages")]),e._v(" "),t("ul",[t("li",[e._v("it reduces clients’ need to interact with the master because reads and writes on the same chunk require only one initial request to the master for chunk location information.")]),e._v(" "),t("li",[e._v("Second, since on a large chunk, a client is more likely to perform many operations on a given chunk, it can reduce network overhead by keeping a persistent TCP connection to the chunkserver over an extended period of time.")]),e._v(" "),t("li",[e._v("Third, it reduces the size of the metadata stored on the master. This allows us to keep")])]),e._v(" "),t("h3",{attrs:{id:"metadata"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#metadata"}},[e._v("#")]),e._v(" Metadata")]),e._v(" "),t("p",[e._v("The master stores three major types of metadata: the file and chunk namespaces, the mapping from files to chunks, and the locations of each chunk’s replicas. All metadata is kept in the master’s memory.")]),e._v(" "),t("p",[t("img",{attrs:{src:a(297),alt:"2.png"}})]),e._v(" "),t("h3",{attrs:{id:"consistency-model"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#consistency-model"}},[e._v("#")]),e._v(" Consistency Model")]),e._v(" "),t("p",[e._v("A relaxed consistency model.")]),e._v(" "),t("p",[e._v("What it does guarantee is that every piece of data stored will be written "),t("em",[e._v("at least once")]),e._v(" on each replica.")]),e._v(" "),t("p",[e._v("A file region is consistent if all clients will always see the same data, regardless of which replicas they read from.")]),e._v(" "),t("p",[e._v("A region is defined after a file data mutation if it is consistent and clients will see what the mutation writes in its entirety.")]),e._v(" "),t("ul",[t("li",[e._v("File namespace mutations (e.g., file creation) are atomic. They are handled exclusively by the master: namespace locking guarantees atomicity and correctness (Section 4.1); the master’s operation log defines a global total order of these operations (Section 2.6.3).")])]),e._v(" "),t("h2",{attrs:{id:"system-interactions"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#system-interactions"}},[e._v("#")]),e._v(" System Interactions")]),e._v(" "),t("p",[t("img",{attrs:{src:a(298),alt:"3.png"}})]),e._v(" "),t("h3",{attrs:{id:"leases-and-mutation-order"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#leases-and-mutation-order"}},[e._v("#")]),e._v(" Leases and Mutation Order")]),e._v(" "),t("p",[e._v("A mutation is an operation that changes the contents or metadata of a chunk such as a write or an append operation.")]),e._v(" "),t("p",[e._v("The master grants a chunk lease to one of the replicas, which we call the primary.")]),e._v(" "),t("p",[e._v("The primary picks a serial order for all mutations to the chunk. All replicas follow this order when applying mutations.")]),e._v(" "),t("ol",[t("li",[e._v("The client asks the master which chunkserver holds the current lease for the chunk and the locations of the other replicas. If no one has a lease, the master grants one to a replica it chooses (not shown).")]),e._v(" "),t("li",[e._v("The master replies with the identity of the primary and the locations of the other (secondary) replicas. The client caches this data for future mutations. It needs to contact the master again only when the primary becomes unreachable or replies that it no longer holds a lease.")]),e._v(" "),t("li",[e._v("The client pushes the data to all the replicas. A client can do so in any order. Each chunkserver will store the data in an internal LRU buffer cache until the data is used or aged out. By decoupling the data flow from the control flow, we can improve performance by scheduling the expensive data flow based on the network topology regardless of which chunkserver is the primary.")]),e._v(" "),t("li",[e._v("Once all the replicas have acknowledged receiving the data, the client sends a write request to the primary. The request identifies the data pushed earlier to all of the replicas. The primary assigns consecutive serial numbers to all the mutations it receives, possibly from multiple clients, which provides the necessary serialization. It applies the mutation to its own local state in serial number order.")]),e._v(" "),t("li",[e._v("The primary forwards the write request to all secondary replicas. Each secondary replica applies mutations in the same serial number order assigned by the primary.")]),e._v(" "),t("li",[e._v("The secondaries all reply to the primary indicating that they have completed the operation.")]),e._v(" "),t("li",[e._v("The primary replies to the client. Any errors encountered at any of the replicas are reported to the client. In case of errors, the write may have succeeded at the primary and an arbitrary subset of the secondary replicas. (If it had failed at the primary, it would not have been assigned a serial number and forwarded.) The client request is considered to have failed, and the modified region is left in an inconsistent state. Our client code handles such errors by retrying the failed mutation. It will make a few attempts at steps (3) through (7) before falling back to a retry from the beginning of the write.")])]),e._v(" "),t("h3",{attrs:{id:"data-flow"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#data-flow"}},[e._v("#")]),e._v(" Data Flow")]),e._v(" "),t("ul",[t("li",[e._v("We decouple the flow of data from the flow of control to use the network efficiently.")]),e._v(" "),t("li",[e._v("To fully utilize each machine’s network bandwidth, the data is pushed linearly along a chain of chunkservers rather than distributed in some other topology (e.g., tree).")]),e._v(" "),t("li",[e._v("To avoid network bottlenecks and high-latency links (e.g., inter-switch links are often both) as much as possible, each machine forwards the data to the “closest” machine in the network topology that has not received it.")]),e._v(" "),t("li",[e._v("We minimize latency by pipelining the data transfer over TCP connections.(a switched network with full-duplex links.)")])]),e._v(" "),t("h3",{attrs:{id:"atomic-record-appends"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#atomic-record-appends"}},[e._v("#")]),e._v(" Atomic Record Appends")]),e._v(" "),t("p",[e._v("GFS does not guarantee that all replicas are "),t("strong",[e._v("bytewise identical")]),e._v(". It only guarantees that the")]),e._v(" "),t("p",[e._v("data is written at least once as an atomic unit.")]),e._v(" "),t("blockquote",[t("ol",[t("li",[e._v("如果遇到多客户端并发，由系统统一安排追加顺序，并且单个记录追加时不会被中断。")]),e._v(" "),t("li",[e._v("如果由于节点或者网络故障导致追加失败，会对记录进行重试，即保证至少写成功一次")])])]),e._v(" "),t("h3",{attrs:{id:"snapshot"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#snapshot"}},[e._v("#")]),e._v(" Snapshot")]),e._v(" "),t("p",[e._v("The snapshot operation makes a copy of a file or a directory tree (the “source”) almost instantaneously, while minimizing any interruptions of ongoing mutations.")]),e._v(" "),t("p",[e._v("Like AFS [5], we use standard "),t("strong",[e._v("copy-on-write")]),e._v(" techniques to implement snapshots.")]),e._v(" "),t("h2",{attrs:{id:"master-operation"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#master-operation"}},[e._v("#")]),e._v(" Master Operation")]),e._v(" "),t("h3",{attrs:{id:"namespace-management-and-locking"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#namespace-management-and-locking"}},[e._v("#")]),e._v(" Namespace Management and Locking")]),e._v(" "),t("p",[e._v("GFS logically represents its namespace as a lookup table mapping full pathnames to metadata.")]),e._v(" "),t("h3",{attrs:{id:"replica-placement"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#replica-placement"}},[e._v("#")]),e._v(" Replica Placement")]),e._v(" "),t("p",[e._v("A GFS cluster is highly distributed at more levels than one.")]),e._v(" "),t("h3",{attrs:{id:"creation-re-replication-rebalancing"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#creation-re-replication-rebalancing"}},[e._v("#")]),e._v(" Creation, Re-replication, Rebalancing")]),e._v(" "),t("p",[e._v("Chunk replicas are created for three reasons: chunk creation, re-replication, and rebalancing.")]),e._v(" "),t("h3",{attrs:{id:"garbage-collection"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#garbage-collection"}},[e._v("#")]),e._v(" Garbage Collection")]),e._v(" "),t("h4",{attrs:{id:"mechanism"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#mechanism"}},[e._v("#")]),e._v(" Mechanism")]),e._v(" "),t("p",[e._v("The file is just renamed to a hidden name that includes the deletion timestamp.")]),e._v(" "),t("h4",{attrs:{id:"discussion"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#discussion"}},[e._v("#")]),e._v(" Discussion")]),e._v(" "),t("p",[e._v("all "),t("strong",[e._v("references to chunks")]),e._v(": they are in the file-to-chunk mappings maintained exclusively by the master.")]),e._v(" "),t("p",[e._v("all "),t("strong",[e._v("the chunk replicas")]),e._v(": they are Linux files under designated directories on each chunkserver.")]),e._v(" "),t("p",[e._v("Any such replica not known to the master is “garbage.”")]),e._v(" "),t("p",[e._v("The garbage collection approach to storage reclamation offers several advantages over eager deletion.")]),e._v(" "),t("h3",{attrs:{id:"stale-replica-detection"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#stale-replica-detection"}},[e._v("#")]),e._v(" Stale Replica Detection")]),e._v(" "),t("p",[e._v("For each chunk, the master maintains a chunk version number to distinguish between up-to-date and stale replicas.")]),e._v(" "),t("h2",{attrs:{id:"fault-tolerance-and-diagnosis"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#fault-tolerance-and-diagnosis"}},[e._v("#")]),e._v(" Fault Tolerance and diagnosis")]),e._v(" "),t("h3",{attrs:{id:"high-availability"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#high-availability"}},[e._v("#")]),e._v(" High Availability")]),e._v(" "),t("h4",{attrs:{id:"fast-recovery"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#fast-recovery"}},[e._v("#")]),e._v(" Fast recovery")]),e._v(" "),t("p",[e._v("Both the master and the chunkserver are designed to restore their state and start in seconds no matter how they terminated.")]),e._v(" "),t("h4",{attrs:{id:"chunk-replication"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#chunk-replication"}},[e._v("#")]),e._v(" Chunk Replication")]),e._v(" "),t("p",[e._v("Each chunk is replicated on multiple chunkservers on different racks.")]),e._v(" "),t("h4",{attrs:{id:"master-replication"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#master-replication"}},[e._v("#")]),e._v(" Master Replication")]),e._v(" "),t("p",[e._v("Its operation log and checkpoints are replicated on multiple machines.")]),e._v(" "),t("h3",{attrs:{id:"data-integrity"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#data-integrity"}},[e._v("#")]),e._v(" Data Integrity")]),e._v(" "),t("p",[e._v("Each chunkserver uses checksumming to detect corruption of stored data.")]),e._v(" "),t("h3",{attrs:{id:"diagnostic-tools"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#diagnostic-tools"}},[e._v("#")]),e._v(" Diagnostic Tools")]),e._v(" "),t("p",[e._v("Extensive and detailed diagnostic logging has helped immeasurably in problem isolation, debugging, and performance analysis, while incurring only a minimal cost.")]),e._v(" "),t("h2",{attrs:{id:"measurement"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#measurement"}},[e._v("#")]),e._v(" Measurement")]),e._v(" "),t("h2",{attrs:{id:"related-work"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#related-work"}},[e._v("#")]),e._v(" Related Work")]),e._v(" "),t("h2",{attrs:{id:"conclusions"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#conclusions"}},[e._v("#")]),e._v(" Conclusions")]),e._v(" "),t("ol",[t("li",[e._v("We treat component failures as the "),t("strong",[e._v("norm")]),e._v(" rather than the exception")]),e._v(" "),t("li",[e._v("Our system provides fault tolerance by "),t("strong",[e._v("constant monitoring")]),e._v(", "),t("strong",[e._v("replicating crucial data")]),e._v(", and "),t("strong",[e._v("fast and automatic recovery")]),e._v(".")]),e._v(" "),t("li",[e._v("Additionally, we use "),t("strong",[e._v("checksumming")]),e._v(" to detect data corruption at the disk or IDE subsystem level, which becomes all too common given the number of disks in the system.")]),e._v(" "),t("li",[e._v("Our design delivers high aggregate throughput to many concurrent readers and writers performing a variety of tasks. We achieve this by "),t("strong",[e._v("separating file system control")]),e._v(", which passes through the master, from data transfer, which passes directly between chunkservers and clients.")]),e._v(" "),t("li",[e._v("Master involvement in common operations is "),t("strong",[e._v("minimized")]),e._v(" by a "),t("strong",[e._v("large chunk size")]),e._v(" and by "),t("strong",[e._v("chunk leases")]),e._v(", which delegates authority to primary replicas in data mutations.")])]),e._v(" "),t("h2",{attrs:{id:"references"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#references"}},[e._v("#")]),e._v(" References")]),e._v(" "),t("ol",[t("li",[t("a",{attrs:{href:"https://juejin.cn/post/7208048755123437626#heading-17",target:"_blank",rel:"noopener noreferrer"}},[e._v("https://juejin.cn/post/7208048755123437626#heading-17"),t("OutboundLink")],1)]),e._v(" "),t("li",[t("a",{attrs:{href:"https://www.zhihu.com/tardis/zm/art/79746847?source_id=1003",target:"_blank",rel:"noopener noreferrer"}},[e._v("https://www.zhihu.com/tardis/zm/art/79746847?source_id=1003"),t("OutboundLink")],1)]),e._v(" "),t("li",[t("a",{attrs:{href:"https://spongecaptain.cool/post/paper/googlefilesystem/",target:"_blank",rel:"noopener noreferrer"}},[e._v("https://spongecaptain.cool/post/paper/googlefilesystem/"),t("OutboundLink")],1)]),e._v(" "),t("li",[e._v("Video: "),t("a",{attrs:{href:"https://www.youtube.com/watch?v=6ETFk1-53qU&feature=youtu.be",target:"_blank",rel:"noopener noreferrer"}},[e._v("https://www.youtube.com/watch?v=6ETFk1-53qU&feature=youtu.be"),t("OutboundLink")],1)]),e._v(" "),t("li",[e._v("Lecture: "),t("a",{attrs:{href:"http://nil.csail.mit.edu/6.824/2022/notes/l-gfs.txt",target:"_blank",rel:"noopener noreferrer"}},[e._v("http://nil.csail.mit.edu/6.824/2022/notes/l-gfs.txt"),t("OutboundLink")],1)]),e._v(" "),t("li",[e._v("FAQ: "),t("a",{attrs:{href:"http://nil.csail.mit.edu/6.824/2022/papers/gfs-faq.txt",target:"_blank",rel:"noopener noreferrer"}},[e._v("http://nil.csail.mit.edu/6.824/2022/papers/gfs-faq.txt"),t("OutboundLink")],1)]),e._v(" "),t("li",[e._v("Paper: "),t("a",{attrs:{href:"http://nil.csail.mit.edu/6.824/2022/papers/gfs.pdf",target:"_blank",rel:"noopener noreferrer"}},[e._v("http://nil.csail.mit.edu/6.824/2022/papers/gfs.pdf"),t("OutboundLink")],1)])]),e._v(" "),t("h2",{attrs:{id:"q-a"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#q-a"}},[e._v("#")]),e._v(" Q & A")]),e._v(" "),t("p",[t("strong",[e._v("Why is atomic record append at-least-once, rather than exactly\nonce?")])]),e._v(" "),t("p",[e._v("Section 3.1, Step 7, says that if a write fails at one of the\nsecondaries, the client re-tries the write. That will cause the data\nto be appended more than once at the non-failed replicas. A different\ndesign could probably detect duplicate client requests despite\narbitrary failures (e.g. a primary failure between the original\nrequest and the client's retry). You'll implement such a design in Lab\n3, at considerable expense in complexity and performance.")]),e._v(" "),t("p",[t("strong",[e._v("How does an application know what sections of a chunk consist of\npadding and duplicate records?")])]),e._v(" "),t("p",[e._v("To detect padding, applications can put a predictable magic number\nat the start of a valid record, or include a checksum that will likely\nonly be valid if the record is valid. The application can detect\nduplicates by including unique IDs in records. Then, if it reads a\nrecord that has the same ID as an earlier record, it knows that they\nare duplicates of each other. GFS provides a library for applications\nthat handles these cases.")]),e._v(" "),t("p",[t("strong",[e._v("How can clients find their data given that atomic record append\nwrites it at an unpredictable offset in the file?")])]),e._v(" "),t("p",[e._v("Append (and GFS in general) is mostly intended for applications\nthat sequentially read entire files. Such applications will scan the\nfile looking for valid records (see the previous question), so they\ndon't need to know the record locations in advance. For example, the\nfile might contain the set of link URLs encountered by a set of\nconcurrent web crawlers. The file offset of any given URL doesn't\nmatter much; readers just want to be able to read the entire set of\nURLs.")]),e._v(" "),t("p",[t("strong",[e._v("What's a checksum?")])]),e._v(" "),t("p",[e._v("A checksum algorithm takes a block of bytes as input and returns a\nsingle number that's a function of all the input bytes. For example, a\nsimple checksum might be the sum of all the bytes in the input (mod\nsome big number). GFS stores the checksum of each chunk as well as the\nchunk. When a chunkserver writes a chunk on its disk, it first\ncomputes the checksum of the new chunk, and saves the checksum on disk\nas well as the chunk. When a chunkserver reads a chunk from disk, it\nalso reads the previously-saved checksum, re-computes a checksum from\nthe chunk read from disk, and checks that the two checksums match. If\nthe data was corrupted by the disk, the checksums won't match, and the\nchunkserver will know to return an error. Separately, some GFS\napplications stored their own checksums, over application-defined\nrecords, inside GFS files, to distinguish between correct records and\npadding. CRC32 is an example of a checksum algorithm.")]),e._v(" "),t("p",[t("strong",[e._v("The paper mentions reference counts -- what are they?")])]),e._v(" "),t("p",[e._v("They are part of the implementation of copy-on-write for snapshots.\nWhen GFS creates a snapshot, it doesn't copy the chunks, but instead\nincreases the reference counter of each chunk. This makes creating a\nsnapshot inexpensive. If a client writes a chunk and the master\nnotices the reference count is greater than one, the master first\nmakes a copy so that the client can update the copy (instead of the\nchunk that is part of the snapshot). You can view this as delaying the\ncopy until it is absolutely necessary. The hope is that not all chunks\nwill be modified and one can avoid making some copies.")]),e._v(" "),t("p",[t("strong",[e._v("If an application uses the standard POSIX file APIs, would it need\nto be modified in order to use GFS?")])]),e._v(" "),t("p",[e._v("Yes, but GFS isn't intended for existing applications. It is\ndesigned for newly-written applications, such as MapReduce programs.")]),e._v(" "),t("p",[t("strong",[e._v("How does GFS determine the location of the nearest replica?")])]),e._v(" "),t("p",[e._v("The paper hints that GFS does this based on the IP addresses of the\nservers storing the available replicas. In 2003, Google must have\nassigned IP addresses in such a way that if two IP addresses are close\nto each other in IP address space, then they are also close together\nin the machine room.")]),e._v(" "),t("p",[t("strong",[e._v("What's a lease?")])]),e._v(" "),t("p",[e._v("For GFS, a lease is a period of time in which a particular\nchunkserver is allowed to be the primary for a particular chunk.\nLeases are a way to avoid having the primary have to repeatedly ask\nthe master if it is still primary -- it knows it can act as primary\nfor the next minute (or whatever the lease interval is) without\ntalking to the master again.")]),e._v(" "),t("p",[t("strong",[e._v("Suppose S1 is the primary for a chunk, and the network between the\nmaster and S1 fails. The master will notice and designate some other\nserver as primary, say S2. Since S1 didn't actually fail, are there\nnow two primaries for the same chunk?")])]),e._v(" "),t("p",[e._v("That would be a disaster, since both primaries might apply\ndifferent updates to the same chunk. Luckily GFS's lease mechanism\nprevents this scenario. The master granted S1 a 60-second lease to be\nprimary. S1 knows to stop being primary before its lease expires. The\nmaster won't grant a lease to S2 until after the previous lease to S1\nexpires. So S2 won't start acting as primary until after S1 stops.")]),e._v(" "),t("p",[t("strong",[e._v("64 megabytes sounds awkwardly large for the chunk size!")])]),e._v(" "),t("p",[e._v("The 64 MB chunk size is the unit of book-keeping in the master, and\nthe granularity at which files are sharded over chunkservers. Clients\ncould issue smaller reads and writes -- they were not forced to deal\nin whole 64 MB chunks. The point of using such a big chunk size is to\nreduce the size of the meta-data tables in the master, and to avoid\nlimiting clients that want to do huge transfers to reduce overhead. On\nthe other hand, files less than 64 MB in size do not get much\nparallelism.")]),e._v(" "),t("p",[t("strong",[e._v("Does Google still use GFS?")])]),e._v(" "),t("p",[e._v("Rumor has it that GFS has been replaced by something called\nColossus, with the same overall goals, but improvements in master\nperformance and fault-tolerance. In addition, many applications within\nGoogle have switched to more database-like storage systems such as\nBigTable and Spanner. However, much of the GFS design lives on in\nHDFS, the storage system for the Hadoop open-source MapReduce.")]),e._v(" "),t("p",[t("strong",[e._v("How acceptable is it that GFS trades correctness for performance\nand simplicity?")])]),e._v(" "),t("p",[e._v("This a recurring theme in distributed systems. Strong consistency\nusually requires protocols that are complex and require chit-chat\nbetween machines (as we will see in the next few lectures). By\nexploiting ways that specific application classes can tolerate relaxed\nconsistency, one can design systems that have good performance and\nsufficient consistency. For example, GFS optimizes for MapReduce\napplications, which need high read performance for large files and are\nOK with having holes in files, records showing up several times, and\ninconsistent reads. On the other hand, GFS would not be good for\nstoring account balances at a bank.")]),e._v(" "),t("p",[t("strong",[e._v("What if the master fails?")])]),e._v(" "),t("p",[e._v("There are replica masters with a full copy of the master state; the\npaper's design requires some outside entity (a human?) to decide to\nswitch to one of the replicas after a master failure (Section 5.1.3).\nWe will see later how to build replicated services with automatic\ncut-over to a backup, using Raft.")]),e._v(" "),t("p",[t("strong",[e._v("Why 3 replicas?")])]),e._v(" "),t("p",[e._v("Perhaps this was the line of reasoning: two replicas are not enough\nbecause, after one fails, there may not be enough time to re-replicate\nbefore the remaining replica fails; three makes that scenario much\nless likely. With 1000s of disks, low-probabilty events like multiple\nreplicas failing in short order occur uncomfortably often. Here is a\nstudy of disk reliability from that era:\nhttps://research.google.com/archive/disk_failures.pdf. You need to\nfactor in the time it takes to make new copies of all the chunks that\nwere stored on a failed disk; and perhaps also the frequency of power,\nserver, network, and software failures. The cost of disks (and\nassociated power, air conditioning, and rent), and the value of the\ndata being protected, are also relevant.")]),e._v(" "),t("p",[t("strong",[e._v("Did having a single master turn out to be a good idea?")])]),e._v(" "),t("p",[e._v("That idea simplified initial deployment but was not so great in the\nlong run. This article (GFS: Evolution on Fast Forward,\nhttps://queue.acm.org/detail.cfm?id=1594206) says that as the years\nwent by and GFS use grew, a few things went wrong. The number of files\ngrew enough that it wasn't reasonable to store all files' metadata in\nthe RAM of a single master. The number of clients grew enough that a\nsingle master didn't have enough CPU power to serve them. The fact\nthat switching from a failed master to one of its backups required\nhuman intervention made recovery slow. Apparently Google's replacement\nfor GFS, Colossus, splits the master over multiple servers, and has\nmore automated master failure recovery.")]),e._v(" "),t("p",[t("strong",[e._v("What is internal fragmentation? Why does lazy allocation help?")])]),e._v(" "),t("p",[e._v("Internal fragmentation is the space wasted when a system uses an\nallocation unit larger than needed for the requested allocation. If\nGFS allocated disk space in 64MB units, then a one-byte chunk would\nwaste almost 64MB of disk. GFS avoids this problem by allocating disk\nspace lazily. Every chunk is a Linux file, and Linux file systems use\nblock sizes of a few tens of kilobytes; so when an application creates\na 1-byte GFS file, the file's chunk consumes only one Linux disk\nblock, not 64 MB.")]),e._v(" "),t("p",[t("strong",[e._v("What benefit does GFS obtain from the weakness of its consistency?")])]),e._v(" "),t("p",[e._v("It's easier to think about the additional work GFS would have to do\nto achieve stronger consistency.")]),e._v(" "),t("p",[e._v("The primary should not let secondaries apply a write unless all\nsecondaries will be able to do it. This likely requires two rounds of\ncommunication -- one to ask all secondaries if they are alive and are\nable to promise to do the write if asked, and (if all answer yes) a\nsecond round to tell the secondaries to commit the write.")]),e._v(" "),t("p",[e._v("If the primary dies, some secondaries may have missed the last few\nmessages the primary sent. Before resuming operation, a new primary\nshould ensure that all the secondaries have seen exactly the same\nsequence of messages.")]),e._v(" "),t("p",[e._v("Since clients re-send requests if they suspect something has gone\nwrong, primaries would need to filter out operations that have already\nbeen executed.")]),e._v(" "),t("p",[e._v("Clients cache chunk locations, and may send reads to a chunkserver\nthat holds a stale version of a chunk. GFS would need a way to\nguarantee that this cannot succeed.")])])}),[],!1,null,null,null);t.default=n.exports}}]);