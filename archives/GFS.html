<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>GFS | Blog Jianping5</title>
    <meta name="generator" content="VuePress 1.9.9">
    <link rel="icon" href="favicon.jpg">
    <meta name="description" content="我的个人博客网站">
    <meta name="theme-color" content="#3eaf7c">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    
    <link rel="preload" href="/assets/css/0.styles.1a8e0fbc.css" as="style"><link rel="preload" href="/assets/js/app.589ac16e.js" as="script"><link rel="preload" href="/assets/js/2.733019b2.js" as="script"><link rel="preload" href="/assets/js/8.f807af7d.js" as="script"><link rel="prefetch" href="/assets/js/10.2a222a01.js"><link rel="prefetch" href="/assets/js/11.78233bd9.js"><link rel="prefetch" href="/assets/js/12.67ff6154.js"><link rel="prefetch" href="/assets/js/3.64801815.js"><link rel="prefetch" href="/assets/js/4.d12742be.js"><link rel="prefetch" href="/assets/js/5.216cae74.js"><link rel="prefetch" href="/assets/js/6.2d0a63f8.js"><link rel="prefetch" href="/assets/js/7.c145f686.js"><link rel="prefetch" href="/assets/js/9.00457b15.js">
    <link rel="stylesheet" href="/assets/css/0.styles.1a8e0fbc.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container no-sidebar"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/" class="home-link router-link-active"><!----> <span class="site-name">Blog Jianping5</span></a> <div class="links"><!----> <nav class="nav-links can-hide"><div class="nav-item"><a href="/archives/" class="nav-link router-link-active">
  Archives
</a></div><div class="nav-item"><a href="/about/" class="nav-link">
  About
</a></div><div class="nav-item"><a href="https://github.com/jianping5" target="_blank" rel="noopener noreferrer" class="nav-link external">
  Github
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <!----></nav></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><nav class="nav-links"><div class="nav-item"><a href="/archives/" class="nav-link router-link-active">
  Archives
</a></div><div class="nav-item"><a href="/about/" class="nav-link">
  About
</a></div><div class="nav-item"><a href="https://github.com/jianping5" target="_blank" rel="noopener noreferrer" class="nav-link external">
  Github
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <!----></nav>  <!----> </aside> <main class="page"> <div class="theme-default-content content__default"><h1 id="gfs"><a href="#gfs" class="header-anchor">#</a> GFS</h1> <p><font size="3" color="gray">Posted on May 11, 2023 (60 min. read)</font></p> <h2 id="design-overview"><a href="#design-overview" class="header-anchor">#</a> Design Overview</h2> <h3 id="architecture"><a href="#architecture" class="header-anchor">#</a> Architecture</h3> <p><img src="https://jp-typora-1310703557.cos.ap-shanghai.myqcloud.com/blog/1280X1280.PNG" alt="1280X1280"></p> <p>A GFS consists of</p> <ul><li>A single master</li> <li>Multiple chunkservers</li> <li>Multiple clients</li></ul> <p>Files are divided into fixed-size chunks.</p> <p>The master maintains all file system <strong>metadata</strong>.</p> <p>GFS client code linked into each application implements the file system API and communicates with the master and chunkservers to read or write data on behalf of the application.</p> <p>Clients interact with the master for metadata operations, but all data-bearing communication goes directly to the chunkservers.</p> <p><strong>Clients do cache metadata</strong>.</p> <h3 id="single-master"><a href="#single-master" class="header-anchor">#</a> Single Master</h3> <p>Having a single master vastly simplifies our design and enables the master to make sophisticated chunk placement and replication decisions using global knowledge.</p> <h3 id="chunk-size"><a href="#chunk-size" class="header-anchor">#</a> Chunk Size</h3> <p>Lazy space allocation avoids wasting space due to internal fragmentation, perhaps the greatest objection against such a large chunk size.</p> <p>A large chunk size offers several important advantages</p> <ul><li>it reduces clients’ need to interact with the master because reads and writes on the same chunk require only one initial request to the master for chunk location information.</li> <li>Second, since on a large chunk, a client is more likely to perform many operations on a given chunk, it can reduce network overhead by keeping a persistent TCP connection to the chunkserver over an extended period of time.</li> <li>Third, it reduces the size of the metadata stored on the master. This allows us to keep</li></ul> <h3 id="metadata"><a href="#metadata" class="header-anchor">#</a> Metadata</h3> <p>The master stores three major types of metadata: the file and chunk namespaces, the mapping from files to chunks, and the locations of each chunk’s replicas. All metadata is kept in the master’s memory.</p> <p><img src="https://jp-typora-1310703557.cos.ap-shanghai.myqcloud.com/blog/e5161e7b-7d90-4598-9426-7edc507163b6.png" alt="e5161e7b-7d90-4598-9426-7edc507163b6"></p> <h3 id="consistency-model"><a href="#consistency-model" class="header-anchor">#</a> Consistency Model</h3> <p>A relaxed consistency model.</p> <p>What it does guarantee is that every piece of data stored will be written <em>at least once</em> on each replica.</p> <p>A file region is consistent if all clients will always see the same data, regardless of which replicas they read from.</p> <p>A region is defined after a file data mutation if it is consistent and clients will see what the mutation writes in its entirety.</p> <ul><li>File namespace mutations (e.g., file creation) are atomic. They are handled exclusively by the master: namespace locking guarantees atomicity and correctness (Section 4.1); the master’s operation log defines a global total order of these operations (Section 2.6.3).</li></ul> <h2 id="system-interactions"><a href="#system-interactions" class="header-anchor">#</a> System Interactions</h2> <p><img src="https://jp-typora-1310703557.cos.ap-shanghai.myqcloud.com/blog/2844e0f0-7444-44ea-97ed-a52ac200a176.png" alt="2844e0f0-7444-44ea-97ed-a52ac200a176"></p> <h3 id="leases-and-mutation-order"><a href="#leases-and-mutation-order" class="header-anchor">#</a> Leases and Mutation Order</h3> <p>A mutation is an operation that changes the contents or metadata of a chunk such as a write or an append operation.</p> <p>The master grants a chunk lease to one of the replicas, which we call the primary.</p> <p>The primary picks a serial order for all mutations to the chunk. All replicas follow this order when applying mutations.</p> <ol><li>The client asks the master which chunkserver holds the current lease for the chunk and the locations of the other replicas. If no one has a lease, the master grants one to a replica it chooses (not shown).</li> <li>The master replies with the identity of the primary and the locations of the other (secondary) replicas. The client caches this data for future mutations. It needs to contact the master again only when the primary becomes unreachable or replies that it no longer holds a lease.</li> <li>The client pushes the data to all the replicas. A client can do so in any order. Each chunkserver will store the data in an internal LRU buffer cache until the data is used or aged out. By decoupling the data flow from the control flow, we can improve performance by scheduling the expensive data flow based on the network topology regardless of which chunkserver is the primary.</li> <li>Once all the replicas have acknowledged receiving the data, the client sends a write request to the primary. The request identifies the data pushed earlier to all of the replicas. The primary assigns consecutive serial numbers to all the mutations it receives, possibly from multiple clients, which provides the necessary serialization. It applies the mutation to its own local state in serial number order.</li> <li>The primary forwards the write request to all secondary replicas. Each secondary replica applies mutations in the same serial number order assigned by the primary.</li> <li>The secondaries all reply to the primary indicating that they have completed the operation.</li> <li>The primary replies to the client. Any errors encountered at any of the replicas are reported to the client. In case of errors, the write may have succeeded at the primary and an arbitrary subset of the secondary replicas. (If it had failed at the primary, it would not have been assigned a serial number and forwarded.) The client request is considered to have failed, and the modified region is left in an inconsistent state. Our client code handles such errors by retrying the failed mutation. It will make a few attempts at steps (3) through (7) before falling back to a retry from the beginning of the write.</li></ol> <h3 id="data-flow"><a href="#data-flow" class="header-anchor">#</a> Data Flow</h3> <ul><li>We decouple the flow of data from the flow of control to use the network efficiently.</li> <li>To fully utilize each machine’s network bandwidth, the data is pushed linearly along a chain of chunkservers rather than distributed in some other topology (e.g., tree).</li> <li>To avoid network bottlenecks and high-latency links (e.g., inter-switch links are often both) as much as possible, each machine forwards the data to the “closest” machine in the network topology that has not received it.</li> <li>We minimize latency by pipelining the data transfer over TCP connections.(a switched network with full-duplex links.)</li></ul> <h3 id="atomic-record-appends"><a href="#atomic-record-appends" class="header-anchor">#</a> Atomic Record Appends</h3> <p>GFS does not guarantee that all replicas are <strong>bytewise identical</strong>. It only guarantees that the</p> <p>data is written at least once as an atomic unit.</p> <blockquote><ol><li>如果遇到多客户端并发，由系统统一安排追加顺序，并且单个记录追加时不会被中断。</li> <li>如果由于节点或者网络故障导致追加失败，会对记录进行重试，即保证至少写成功一次</li></ol></blockquote> <h3 id="snapshot"><a href="#snapshot" class="header-anchor">#</a> Snapshot</h3> <p>The snapshot operation makes a copy of a file or a directory tree (the “source”) almost instantaneously, while minimizing any interruptions of ongoing mutations.</p> <p>Like AFS [5], we use standard <strong>copy-on-write</strong> techniques to implement snapshots.</p> <h2 id="master-operation"><a href="#master-operation" class="header-anchor">#</a> Master Operation</h2> <h3 id="namespace-management-and-locking"><a href="#namespace-management-and-locking" class="header-anchor">#</a> Namespace Management and Locking</h3> <p>GFS logically represents its namespace as a lookup table mapping full pathnames to metadata.</p> <h3 id="replica-placement"><a href="#replica-placement" class="header-anchor">#</a> Replica Placement</h3> <p>A GFS cluster is highly distributed at more levels than one.</p> <h3 id="creation-re-replication-rebalancing"><a href="#creation-re-replication-rebalancing" class="header-anchor">#</a> Creation, Re-replication, Rebalancing</h3> <p>Chunk replicas are created for three reasons: chunk creation, re-replication, and rebalancing.</p> <h3 id="garbage-collection"><a href="#garbage-collection" class="header-anchor">#</a> Garbage Collection</h3> <h4 id="mechanism"><a href="#mechanism" class="header-anchor">#</a> Mechanism</h4> <p>The file is just renamed to a hidden name that includes the deletion timestamp.</p> <h4 id="discussion"><a href="#discussion" class="header-anchor">#</a> Discussion</h4> <p>all <strong>references to chunks</strong>: they are in the file-to-chunk mappings maintained exclusively by the master.</p> <p>all <strong>the chunk replicas</strong>: they are Linux files under designated directories on each chunkserver.</p> <p>Any such replica not known to the master is “garbage.”</p> <p>The garbage collection approach to storage reclamation offers several advantages over eager deletion.</p> <h3 id="stale-replica-detection"><a href="#stale-replica-detection" class="header-anchor">#</a> Stale Replica Detection</h3> <p>For each chunk, the master maintains a chunk version number to distinguish between up-to-date and stale replicas.</p> <h2 id="fault-tolerance-and-diagnosis"><a href="#fault-tolerance-and-diagnosis" class="header-anchor">#</a> Fault Tolerance and diagnosis</h2> <h3 id="high-availability"><a href="#high-availability" class="header-anchor">#</a> High Availability</h3> <h4 id="fast-recovery"><a href="#fast-recovery" class="header-anchor">#</a> Fast recovery</h4> <p>Both the master and the chunkserver are designed to restore their state and start in seconds no matter how they terminated.</p> <h4 id="chunk-replication"><a href="#chunk-replication" class="header-anchor">#</a> Chunk Replication</h4> <p>Each chunk is replicated on multiple chunkservers on different racks.</p> <h4 id="master-replication"><a href="#master-replication" class="header-anchor">#</a> Master Replication</h4> <p>Its operation log and checkpoints are replicated on multiple machines.</p> <h3 id="data-integrity"><a href="#data-integrity" class="header-anchor">#</a> Data Integrity</h3> <p>Each chunkserver uses checksumming to detect corruption of stored data.</p> <h3 id="diagnostic-tools"><a href="#diagnostic-tools" class="header-anchor">#</a> Diagnostic Tools</h3> <p>Extensive and detailed diagnostic logging has helped immeasurably in problem isolation, debugging, and performance analysis, while incurring only a minimal cost.</p> <h2 id="measurement"><a href="#measurement" class="header-anchor">#</a> Measurement</h2> <h2 id="related-work"><a href="#related-work" class="header-anchor">#</a> Related Work</h2> <h2 id="conclusions"><a href="#conclusions" class="header-anchor">#</a> Conclusions</h2> <ol><li>We treat component failures as the <strong>norm</strong> rather than the exception</li> <li>Our system provides fault tolerance by <strong>constant monitoring</strong>, <strong>replicating crucial data</strong>, and <strong>fast and automatic recovery</strong>.</li> <li>Additionally, we use <strong>checksumming</strong> to detect data corruption at the disk or IDE subsystem level, which becomes all too common given the number of disks in the system.</li> <li>Our design delivers high aggregate throughput to many concurrent readers and writers performing a variety of tasks. We achieve this by <strong>separating file system control</strong>, which passes through the master, from data transfer, which passes directly between chunkservers and clients.</li> <li>Master involvement in common operations is <strong>minimized</strong> by a <strong>large chunk size</strong> and by <strong>chunk leases</strong>, which delegates authority to primary replicas in data mutations.</li></ol> <h2 id="references"><a href="#references" class="header-anchor">#</a> References</h2> <ol><li><a href="https://juejin.cn/post/7208048755123437626#heading-17" target="_blank" rel="noopener noreferrer">https://juejin.cn/post/7208048755123437626#heading-17<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li> <li><a href="https://www.zhihu.com/tardis/zm/art/79746847?source_id=1003" target="_blank" rel="noopener noreferrer">https://www.zhihu.com/tardis/zm/art/79746847?source_id=1003<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li> <li><a href="https://spongecaptain.cool/post/paper/googlefilesystem/" target="_blank" rel="noopener noreferrer">https://spongecaptain.cool/post/paper/googlefilesystem/<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li> <li>Video: <a href="https://www.youtube.com/watch?v=6ETFk1-53qU&amp;feature=youtu.be" target="_blank" rel="noopener noreferrer">https://www.youtube.com/watch?v=6ETFk1-53qU&amp;feature=youtu.be<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li> <li>Lecture: <a href="http://nil.csail.mit.edu/6.824/2022/notes/l-gfs.txt" target="_blank" rel="noopener noreferrer">http://nil.csail.mit.edu/6.824/2022/notes/l-gfs.txt<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li> <li>FAQ: <a href="http://nil.csail.mit.edu/6.824/2022/papers/gfs-faq.txt" target="_blank" rel="noopener noreferrer">http://nil.csail.mit.edu/6.824/2022/papers/gfs-faq.txt<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li> <li>Paper: <a href="http://nil.csail.mit.edu/6.824/2022/papers/gfs.pdf" target="_blank" rel="noopener noreferrer">http://nil.csail.mit.edu/6.824/2022/papers/gfs.pdf<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li></ol> <h2 id="q-a"><a href="#q-a" class="header-anchor">#</a> Q &amp; A</h2> <p><strong>Why is atomic record append at-least-once, rather than exactly
once?</strong></p> <p>Section 3.1, Step 7, says that if a write fails at one of the
secondaries, the client re-tries the write. That will cause the data
to be appended more than once at the non-failed replicas. A different
design could probably detect duplicate client requests despite
arbitrary failures (e.g. a primary failure between the original
request and the client's retry). You'll implement such a design in Lab
3, at considerable expense in complexity and performance.</p> <p><strong>How does an application know what sections of a chunk consist of
padding and duplicate records?</strong></p> <p>To detect padding, applications can put a predictable magic number
at the start of a valid record, or include a checksum that will likely
only be valid if the record is valid. The application can detect
duplicates by including unique IDs in records. Then, if it reads a
record that has the same ID as an earlier record, it knows that they
are duplicates of each other. GFS provides a library for applications
that handles these cases.</p> <p><strong>How can clients find their data given that atomic record append
writes it at an unpredictable offset in the file?</strong></p> <p>Append (and GFS in general) is mostly intended for applications
that sequentially read entire files. Such applications will scan the
file looking for valid records (see the previous question), so they
don't need to know the record locations in advance. For example, the
file might contain the set of link URLs encountered by a set of
concurrent web crawlers. The file offset of any given URL doesn't
matter much; readers just want to be able to read the entire set of
URLs.</p> <p><strong>What's a checksum?</strong></p> <p>A checksum algorithm takes a block of bytes as input and returns a
single number that's a function of all the input bytes. For example, a
simple checksum might be the sum of all the bytes in the input (mod
some big number). GFS stores the checksum of each chunk as well as the
chunk. When a chunkserver writes a chunk on its disk, it first
computes the checksum of the new chunk, and saves the checksum on disk
as well as the chunk. When a chunkserver reads a chunk from disk, it
also reads the previously-saved checksum, re-computes a checksum from
the chunk read from disk, and checks that the two checksums match. If
the data was corrupted by the disk, the checksums won't match, and the
chunkserver will know to return an error. Separately, some GFS
applications stored their own checksums, over application-defined
records, inside GFS files, to distinguish between correct records and
padding. CRC32 is an example of a checksum algorithm.</p> <p><strong>The paper mentions reference counts -- what are they?</strong></p> <p>They are part of the implementation of copy-on-write for snapshots.
When GFS creates a snapshot, it doesn't copy the chunks, but instead
increases the reference counter of each chunk. This makes creating a
snapshot inexpensive. If a client writes a chunk and the master
notices the reference count is greater than one, the master first
makes a copy so that the client can update the copy (instead of the
chunk that is part of the snapshot). You can view this as delaying the
copy until it is absolutely necessary. The hope is that not all chunks
will be modified and one can avoid making some copies.</p> <p><strong>If an application uses the standard POSIX file APIs, would it need
to be modified in order to use GFS?</strong></p> <p>Yes, but GFS isn't intended for existing applications. It is
designed for newly-written applications, such as MapReduce programs.</p> <p><strong>How does GFS determine the location of the nearest replica?</strong></p> <p>The paper hints that GFS does this based on the IP addresses of the
servers storing the available replicas. In 2003, Google must have
assigned IP addresses in such a way that if two IP addresses are close
to each other in IP address space, then they are also close together
in the machine room.</p> <p><strong>What's a lease?</strong></p> <p>For GFS, a lease is a period of time in which a particular
chunkserver is allowed to be the primary for a particular chunk.
Leases are a way to avoid having the primary have to repeatedly ask
the master if it is still primary -- it knows it can act as primary
for the next minute (or whatever the lease interval is) without
talking to the master again.</p> <p><strong>Suppose S1 is the primary for a chunk, and the network between the
master and S1 fails. The master will notice and designate some other
server as primary, say S2. Since S1 didn't actually fail, are there
now two primaries for the same chunk?</strong></p> <p>That would be a disaster, since both primaries might apply
different updates to the same chunk. Luckily GFS's lease mechanism
prevents this scenario. The master granted S1 a 60-second lease to be
primary. S1 knows to stop being primary before its lease expires. The
master won't grant a lease to S2 until after the previous lease to S1
expires. So S2 won't start acting as primary until after S1 stops.</p> <p><strong>64 megabytes sounds awkwardly large for the chunk size!</strong></p> <p>The 64 MB chunk size is the unit of book-keeping in the master, and
the granularity at which files are sharded over chunkservers. Clients
could issue smaller reads and writes -- they were not forced to deal
in whole 64 MB chunks. The point of using such a big chunk size is to
reduce the size of the meta-data tables in the master, and to avoid
limiting clients that want to do huge transfers to reduce overhead. On
the other hand, files less than 64 MB in size do not get much
parallelism.</p> <p><strong>Does Google still use GFS?</strong></p> <p>Rumor has it that GFS has been replaced by something called
Colossus, with the same overall goals, but improvements in master
performance and fault-tolerance. In addition, many applications within
Google have switched to more database-like storage systems such as
BigTable and Spanner. However, much of the GFS design lives on in
HDFS, the storage system for the Hadoop open-source MapReduce.</p> <p><strong>How acceptable is it that GFS trades correctness for performance
and simplicity?</strong></p> <p>This a recurring theme in distributed systems. Strong consistency
usually requires protocols that are complex and require chit-chat
between machines (as we will see in the next few lectures). By
exploiting ways that specific application classes can tolerate relaxed
consistency, one can design systems that have good performance and
sufficient consistency. For example, GFS optimizes for MapReduce
applications, which need high read performance for large files and are
OK with having holes in files, records showing up several times, and
inconsistent reads. On the other hand, GFS would not be good for
storing account balances at a bank.</p> <p><strong>What if the master fails?</strong></p> <p>There are replica masters with a full copy of the master state; the
paper's design requires some outside entity (a human?) to decide to
switch to one of the replicas after a master failure (Section 5.1.3).
We will see later how to build replicated services with automatic
cut-over to a backup, using Raft.</p> <p><strong>Why 3 replicas?</strong></p> <p>Perhaps this was the line of reasoning: two replicas are not enough
because, after one fails, there may not be enough time to re-replicate
before the remaining replica fails; three makes that scenario much
less likely. With 1000s of disks, low-probabilty events like multiple
replicas failing in short order occur uncomfortably often. Here is a
study of disk reliability from that era:
https://research.google.com/archive/disk_failures.pdf. You need to
factor in the time it takes to make new copies of all the chunks that
were stored on a failed disk; and perhaps also the frequency of power,
server, network, and software failures. The cost of disks (and
associated power, air conditioning, and rent), and the value of the
data being protected, are also relevant.</p> <p><strong>Did having a single master turn out to be a good idea?</strong></p> <p>That idea simplified initial deployment but was not so great in the
long run. This article (GFS: Evolution on Fast Forward,
https://queue.acm.org/detail.cfm?id=1594206) says that as the years
went by and GFS use grew, a few things went wrong. The number of files
grew enough that it wasn't reasonable to store all files' metadata in
the RAM of a single master. The number of clients grew enough that a
single master didn't have enough CPU power to serve them. The fact
that switching from a failed master to one of its backups required
human intervention made recovery slow. Apparently Google's replacement
for GFS, Colossus, splits the master over multiple servers, and has
more automated master failure recovery.</p> <p><strong>What is internal fragmentation? Why does lazy allocation help?</strong></p> <p>Internal fragmentation is the space wasted when a system uses an
allocation unit larger than needed for the requested allocation. If
GFS allocated disk space in 64MB units, then a one-byte chunk would
waste almost 64MB of disk. GFS avoids this problem by allocating disk
space lazily. Every chunk is a Linux file, and Linux file systems use
block sizes of a few tens of kilobytes; so when an application creates
a 1-byte GFS file, the file's chunk consumes only one Linux disk
block, not 64 MB.</p> <p><strong>What benefit does GFS obtain from the weakness of its consistency?</strong></p> <p>It's easier to think about the additional work GFS would have to do
to achieve stronger consistency.</p> <p>The primary should not let secondaries apply a write unless all
secondaries will be able to do it. This likely requires two rounds of
communication -- one to ask all secondaries if they are alive and are
able to promise to do the write if asked, and (if all answer yes) a
second round to tell the secondaries to commit the write.</p> <p>If the primary dies, some secondaries may have missed the last few
messages the primary sent. Before resuming operation, a new primary
should ensure that all the secondaries have seen exactly the same
sequence of messages.</p> <p>Since clients re-send requests if they suspect something has gone
wrong, primaries would need to filter out operations that have already
been executed.</p> <p>Clients cache chunk locations, and may send reads to a chunkserver
that holds a stale version of a chunk. GFS would need a way to
guarantee that this cannot succeed.</p></div> <footer class="page-edit"><!----> <!----></footer> <!----> </main></div><div class="global-ui"></div></div>
    <script src="/assets/js/app.589ac16e.js" defer></script><script src="/assets/js/2.733019b2.js" defer></script><script src="/assets/js/8.f807af7d.js" defer></script>
  </body>
</html>
